{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'compat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b51ef48abedb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompile_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixed_precision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlso\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m   \u001b[0mscipy_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[0mpd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;31m# GH 27101\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;31m# TODO: remove Panel compat in 1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY37\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'compat'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1*X GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=(1024*4))])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-17bb7203622b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_gpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('data\\legend.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_count(labels_df, column_name):\n",
    "    for label in labels_df[column_name].unique():\n",
    "        print('{}: {}'.format(label, len(labels_df[labels_df[column_name] == label])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['emotion'] = labels_df['emotion'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.drop( labels_df[ labels_df['emotion'] == 'fear' ].index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.drop( labels_df[ labels_df['emotion'] == 'contempt' ].index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 252\n",
      "surprise: 368\n",
      "disgust: 208\n",
      "neutral: 6868\n",
      "happiness: 5696\n",
      "sadness: 268\n"
     ]
    }
   ],
   "source": [
    "get_labels_count(labels_df, 'emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.drop(columns=['user.id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = defaultdict(list)\n",
    "for label in labels_df['emotion'].unique():\n",
    "    new_label = label\n",
    "    if label == 'happiness':\n",
    "        new_label = 'positive'\n",
    "    elif label in ['anger', 'sadness', 'disgust']:\n",
    "        new_label = 'negative'\n",
    "    elif label == 'surprise':\n",
    "        continue\n",
    "    label_dict[new_label].extend(labels_df[labels_df['emotion']==label]['image'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_dict['neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_split=defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, images_files in label_dict.items():\n",
    "    np.random.shuffle(images_files)\n",
    "    training_split = int(len(images_files) * 0.8)\n",
    "    training, test = images_files[:training_split], images_files[training_split:]\n",
    "    labels_split[label]['training'] = training\n",
    "    labels_split[label]['testing'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_split=defaultdict(lambda : defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative', 'neutral', 'positive'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_split.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a90c13d1cbd4b8eaed4961473533451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=582.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a902d7e6b8864ca083f7f42409061442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=146.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcd4a9d3a554a5cb8d1a1ee289e92ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5494.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1561e540e1400996199f3df3b6d7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1374.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362d1644a6834202ac80a161996ebe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4556.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109afe412eaf4bd1aa1d03be0bb1b37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1140.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for label in labels_split:\n",
    "    \n",
    "    for split in ['training','testing']:\n",
    "        for img in tqdm(labels_split[label][split]):\n",
    "            if len(images_split[label][split]) >= 1000:\n",
    "                break\n",
    "            img_path = 'images\\{}'.format(img)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            img = np.expand_dims(img, -1)\n",
    "            img = img / 255\n",
    "            \n",
    "            images_split[label][split].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_split['negative']['testing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for label in labels_split:\n",
    "    for split in labels_split[label]:\n",
    "        print(len(labels_split[label][split]) == len(images_split[label][split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_idx = {label: i for i, label in enumerate(labels_split)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0, 'neutral': 1, 'positive': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training =[]\n",
    "x_testing=[]\n",
    "y_training = []\n",
    "y_testing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_split:\n",
    "    x_training +=  images_split[label]['training']\n",
    "    y_training += [labels_to_idx[label]] * len(images_split[label]['training'])\n",
    "    x_testing +=  images_split[label]['testing']\n",
    "    y_testing += [labels_to_idx[label]] * len(images_split[label]['testing'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582, 2146)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_training), len(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2582, 2146)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_training), len(y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = np.array(y_training)\n",
    "y_training_encoded = np.zeros((y_training.size, y_training.max()+1))\n",
    "y_training_encoded[np.arange(y_training.size),y_training] = 1\n",
    "\n",
    "y_testing = np.array(y_testing)\n",
    "y_testing_encoded = np.zeros((y_testing.size, y_testing.max()+1))\n",
    "y_testing_encoded[np.arange(y_testing.size),y_testing] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training = np.array(x_training)\n",
    "x_testing = np.array(x_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2582, 128, 128, 1), (2146, 128, 128, 1), (2582, 3), (2146, 3))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training.shape, x_testing.shape, y_training_encoded.shape, y_testing_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_count = Counter(y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 582, 1: 1000, 2: 1000})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1220c2bcfc8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df['emotion'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_label = max(labels_count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {k: mx_label/v for k, v in labels_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.20784314],\n",
       "        [0.19215686],\n",
       "        [0.16862745],\n",
       "        ...,\n",
       "        [0.57254902],\n",
       "        [0.56862745],\n",
       "        [0.55294118]],\n",
       "\n",
       "       [[0.20784314],\n",
       "        [0.19215686],\n",
       "        [0.17254902],\n",
       "        ...,\n",
       "        [0.55686275],\n",
       "        [0.54901961],\n",
       "        [0.52941176]],\n",
       "\n",
       "       [[0.20784314],\n",
       "        [0.19215686],\n",
       "        [0.16862745],\n",
       "        ...,\n",
       "        [0.5254902 ],\n",
       "        [0.51764706],\n",
       "        [0.49411765]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.2       ],\n",
       "        [0.18823529],\n",
       "        [0.18039216],\n",
       "        ...,\n",
       "        [0.21568627],\n",
       "        [0.22745098],\n",
       "        [0.24705882]],\n",
       "\n",
       "       [[0.2       ],\n",
       "        [0.19215686],\n",
       "        [0.18431373],\n",
       "        ...,\n",
       "        [0.22745098],\n",
       "        [0.23921569],\n",
       "        [0.25098039]],\n",
       "\n",
       "       [[0.2       ],\n",
       "        [0.19215686],\n",
       "        [0.18823529],\n",
       "        ...,\n",
       "        [0.23529412],\n",
       "        [0.24313725],\n",
       "        [0.25490196]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 128, 128, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 688,451\n",
      "Trainable params: 687,971\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        \n",
    "    tf.keras.layers.InputLayer(input_shape=(128, 128, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), padding='same'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('models/{epoch:02d}-{val_loss:.3f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2582 samples, validate on 2146 samples\n",
      "Epoch 1/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 4.4679 - accuracy: 0.4153\n",
      "Epoch 00001: val_loss improved from inf to 3.59484, saving model to models/01-3.595.hdf5\n",
      "2582/2582 [==============================] - 2s 833us/sample - loss: 4.4100 - accuracy: 0.4167 - val_loss: 3.5948 - val_accuracy: 0.2139\n",
      "Epoch 2/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 3.2781 - accuracy: 0.4692\n",
      "Epoch 00002: val_loss did not improve from 3.59484\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 3.2677 - accuracy: 0.4702 - val_loss: 4.1008 - val_accuracy: 0.0750\n",
      "Epoch 3/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 2.9002 - accuracy: 0.4955\n",
      "Epoch 00003: val_loss improved from 3.59484 to 3.43769, saving model to models/03-3.438.hdf5\n",
      "2582/2582 [==============================] - 1s 446us/sample - loss: 2.8905 - accuracy: 0.4973 - val_loss: 3.4377 - val_accuracy: 0.1505\n",
      "Epoch 4/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 2.5737 - accuracy: 0.5259\n",
      "Epoch 00004: val_loss improved from 3.43769 to 3.21430, saving model to models/04-3.214.hdf5\n",
      "2582/2582 [==============================] - 1s 444us/sample - loss: 2.5659 - accuracy: 0.5244 - val_loss: 3.2143 - val_accuracy: 0.0853\n",
      "Epoch 5/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 2.2813 - accuracy: 0.5699\n",
      "Epoch 00005: val_loss improved from 3.21430 to 2.39728, saving model to models/05-2.397.hdf5\n",
      "2582/2582 [==============================] - 1s 443us/sample - loss: 2.2726 - accuracy: 0.5716 - val_loss: 2.3973 - val_accuracy: 0.4138\n",
      "Epoch 6/200\n",
      "2560/2582 [============================>.] - ETA: 0s - loss: 2.0382 - accuracy: 0.6355\n",
      "Epoch 00006: val_loss improved from 2.39728 to 2.14894, saving model to models/06-2.149.hdf5\n",
      "2582/2582 [==============================] - 1s 451us/sample - loss: 2.0372 - accuracy: 0.6359 - val_loss: 2.1489 - val_accuracy: 0.3495\n",
      "Epoch 7/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.8152 - accuracy: 0.6579\n",
      "Epoch 00007: val_loss did not improve from 2.14894\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 1.8088 - accuracy: 0.6580 - val_loss: 2.7210 - val_accuracy: 0.0680\n",
      "Epoch 8/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.6781 - accuracy: 0.6772\n",
      "Epoch 00008: val_loss improved from 2.14894 to 1.93952, saving model to models/08-1.940.hdf5\n",
      "2582/2582 [==============================] - 1s 449us/sample - loss: 1.6700 - accuracy: 0.6778 - val_loss: 1.9395 - val_accuracy: 0.4664\n",
      "Epoch 9/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.4916 - accuracy: 0.7245\n",
      "Epoch 00009: val_loss improved from 1.93952 to 1.74215, saving model to models/09-1.742.hdf5\n",
      "2582/2582 [==============================] - 1s 450us/sample - loss: 1.4906 - accuracy: 0.7215 - val_loss: 1.7421 - val_accuracy: 0.4660\n",
      "Epoch 10/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.3498 - accuracy: 0.7311\n",
      "Epoch 00010: val_loss improved from 1.74215 to 1.59752, saving model to models/10-1.598.hdf5\n",
      "2582/2582 [==============================] - 1s 448us/sample - loss: 1.3462 - accuracy: 0.7320 - val_loss: 1.5975 - val_accuracy: 0.5349\n",
      "Epoch 11/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.2236 - accuracy: 0.7681\n",
      "Epoch 00011: val_loss improved from 1.59752 to 1.48273, saving model to models/11-1.483.hdf5\n",
      "2582/2582 [==============================] - 1s 448us/sample - loss: 1.2176 - accuracy: 0.7692 - val_loss: 1.4827 - val_accuracy: 0.6188\n",
      "Epoch 12/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.1401 - accuracy: 0.7763\n",
      "Epoch 00012: val_loss did not improve from 1.48273\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 1.1397 - accuracy: 0.7769 - val_loss: 1.6101 - val_accuracy: 0.4520\n",
      "Epoch 13/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 1.0218 - accuracy: 0.8030\n",
      "Epoch 00013: val_loss improved from 1.48273 to 1.33334, saving model to models/13-1.333.hdf5\n",
      "2582/2582 [==============================] - 1s 445us/sample - loss: 1.0269 - accuracy: 0.8025 - val_loss: 1.3333 - val_accuracy: 0.6533\n",
      "Epoch 14/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.9484 - accuracy: 0.8199\n",
      "Epoch 00014: val_loss improved from 1.33334 to 1.31986, saving model to models/14-1.320.hdf5\n",
      "2582/2582 [==============================] - 1s 448us/sample - loss: 0.9438 - accuracy: 0.8218 - val_loss: 1.3199 - val_accuracy: 0.6500\n",
      "Epoch 15/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.8818 - accuracy: 0.8405\n",
      "Epoch 00015: val_loss improved from 1.31986 to 1.20859, saving model to models/15-1.209.hdf5\n",
      "2582/2582 [==============================] - 1s 446us/sample - loss: 0.8730 - accuracy: 0.8443 - val_loss: 1.2086 - val_accuracy: 0.7004\n",
      "Epoch 16/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.7764 - accuracy: 0.8618\n",
      "Epoch 00016: val_loss did not improve from 1.20859\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.7726 - accuracy: 0.8637 - val_loss: 1.3404 - val_accuracy: 0.5876\n",
      "Epoch 17/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.7446 - accuracy: 0.8688\n",
      "Epoch 00017: val_loss did not improve from 1.20859\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.7388 - accuracy: 0.8706 - val_loss: 1.2842 - val_accuracy: 0.6710\n",
      "Epoch 18/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.7155 - accuracy: 0.8729\n",
      "Epoch 00018: val_loss improved from 1.20859 to 1.14946, saving model to models/18-1.149.hdf5\n",
      "2582/2582 [==============================] - 1s 444us/sample - loss: 0.7104 - accuracy: 0.8749 - val_loss: 1.1495 - val_accuracy: 0.6626\n",
      "Epoch 19/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.6220 - accuracy: 0.9025\n",
      "Epoch 00019: val_loss improved from 1.14946 to 1.11009, saving model to models/19-1.110.hdf5\n",
      "2582/2582 [==============================] - 1s 457us/sample - loss: 0.6250 - accuracy: 0.8997 - val_loss: 1.1101 - val_accuracy: 0.6682\n",
      "Epoch 20/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.5491 - accuracy: 0.9239\n",
      "Epoch 00020: val_loss improved from 1.11009 to 0.99197, saving model to models/20-0.992.hdf5\n",
      "2582/2582 [==============================] - 1s 448us/sample - loss: 0.5475 - accuracy: 0.9245 - val_loss: 0.9920 - val_accuracy: 0.7563\n",
      "Epoch 21/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.5164 - accuracy: 0.9276\n",
      "Epoch 00021: val_loss did not improve from 0.99197\n",
      "2582/2582 [==============================] - 1s 430us/sample - loss: 0.5163 - accuracy: 0.9280 - val_loss: 1.1211 - val_accuracy: 0.6393\n",
      "Epoch 22/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4687 - accuracy: 0.9441\n",
      "Epoch 00022: val_loss did not improve from 0.99197\n",
      "2582/2582 [==============================] - 1s 439us/sample - loss: 0.4770 - accuracy: 0.9407 - val_loss: 1.0985 - val_accuracy: 0.6626\n",
      "Epoch 23/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4614 - accuracy: 0.9461\n",
      "Epoch 00023: val_loss improved from 0.99197 to 0.92775, saving model to models/23-0.928.hdf5\n",
      "2582/2582 [==============================] - 1s 477us/sample - loss: 0.4573 - accuracy: 0.9473 - val_loss: 0.9277 - val_accuracy: 0.7768\n",
      "Epoch 24/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4735 - accuracy: 0.9449\n",
      "Epoch 00024: val_loss did not improve from 0.92775\n",
      "2582/2582 [==============================] - 1s 429us/sample - loss: 0.4749 - accuracy: 0.9454 - val_loss: 0.9332 - val_accuracy: 0.7526\n",
      "Epoch 25/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4494 - accuracy: 0.9465\n",
      "Epoch 00025: val_loss improved from 0.92775 to 0.86665, saving model to models/25-0.867.hdf5\n",
      "2582/2582 [==============================] - 1s 446us/sample - loss: 0.4464 - accuracy: 0.9477 - val_loss: 0.8666 - val_accuracy: 0.7889\n",
      "Epoch 26/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3705 - accuracy: 0.9749\n",
      "Epoch 00026: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3726 - accuracy: 0.9725 - val_loss: 0.8939 - val_accuracy: 0.7647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3754 - accuracy: 0.9626\n",
      "Epoch 00027: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3797 - accuracy: 0.9620 - val_loss: 0.9117 - val_accuracy: 0.7488\n",
      "Epoch 28/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3536 - accuracy: 0.9683\n",
      "Epoch 00028: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3512 - accuracy: 0.9694 - val_loss: 1.0073 - val_accuracy: 0.7605\n",
      "Epoch 29/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3409 - accuracy: 0.9716\n",
      "Epoch 00029: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3398 - accuracy: 0.9717 - val_loss: 0.9877 - val_accuracy: 0.7647\n",
      "Epoch 30/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2982 - accuracy: 0.9762\n",
      "Epoch 00030: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.2985 - accuracy: 0.9756 - val_loss: 0.8942 - val_accuracy: 0.7707\n",
      "Epoch 31/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3215 - accuracy: 0.9671\n",
      "Epoch 00031: val_loss did not improve from 0.86665\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3279 - accuracy: 0.9663 - val_loss: 0.8999 - val_accuracy: 0.7554\n",
      "Epoch 32/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3223 - accuracy: 0.9675\n",
      "Epoch 00032: val_loss improved from 0.86665 to 0.82465, saving model to models/32-0.825.hdf5\n",
      "2582/2582 [==============================] - 1s 447us/sample - loss: 0.3235 - accuracy: 0.9671 - val_loss: 0.8246 - val_accuracy: 0.8094\n",
      "Epoch 33/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2884 - accuracy: 0.9790\n",
      "Epoch 00033: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2875 - accuracy: 0.9791 - val_loss: 0.9841 - val_accuracy: 0.7726\n",
      "Epoch 34/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2646 - accuracy: 0.9836\n",
      "Epoch 00034: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.2619 - accuracy: 0.9845 - val_loss: 1.0750 - val_accuracy: 0.7721\n",
      "Epoch 35/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2567 - accuracy: 0.9836\n",
      "Epoch 00035: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2538 - accuracy: 0.9845 - val_loss: 1.0440 - val_accuracy: 0.7759\n",
      "Epoch 36/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2464 - accuracy: 0.9844\n",
      "Epoch 00036: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.2479 - accuracy: 0.9841 - val_loss: 0.9728 - val_accuracy: 0.8010\n",
      "Epoch 37/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2332 - accuracy: 0.9864\n",
      "Epoch 00037: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 423us/sample - loss: 0.2319 - accuracy: 0.9868 - val_loss: 1.0176 - val_accuracy: 0.8034\n",
      "Epoch 38/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2144 - accuracy: 0.9889\n",
      "Epoch 00038: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.2134 - accuracy: 0.9892 - val_loss: 1.0629 - val_accuracy: 0.7829\n",
      "Epoch 39/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2003 - accuracy: 0.9885\n",
      "Epoch 00039: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1998 - accuracy: 0.9888 - val_loss: 1.1262 - val_accuracy: 0.7759\n",
      "Epoch 40/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1771 - accuracy: 0.9926\n",
      "Epoch 00040: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.1778 - accuracy: 0.9926 - val_loss: 0.9655 - val_accuracy: 0.7908\n",
      "Epoch 41/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1907 - accuracy: 0.9831\n",
      "Epoch 00041: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 430us/sample - loss: 0.1913 - accuracy: 0.9833 - val_loss: 1.0710 - val_accuracy: 0.7372\n",
      "Epoch 42/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1999 - accuracy: 0.9811\n",
      "Epoch 00042: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1984 - accuracy: 0.9818 - val_loss: 1.0359 - val_accuracy: 0.7670\n",
      "Epoch 43/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2028 - accuracy: 0.9844\n",
      "Epoch 00043: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2021 - accuracy: 0.9849 - val_loss: 1.2810 - val_accuracy: 0.7409\n",
      "Epoch 44/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2004 - accuracy: 0.9889\n",
      "Epoch 00044: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2055 - accuracy: 0.9864 - val_loss: 1.1231 - val_accuracy: 0.8052\n",
      "Epoch 45/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2576 - accuracy: 0.9708\n",
      "Epoch 00045: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2615 - accuracy: 0.9698 - val_loss: 1.1437 - val_accuracy: 0.7833\n",
      "Epoch 46/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3455 - accuracy: 0.9515\n",
      "Epoch 00046: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3460 - accuracy: 0.9520 - val_loss: 1.0878 - val_accuracy: 0.8043\n",
      "Epoch 47/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3367 - accuracy: 0.9683\n",
      "Epoch 00047: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3340 - accuracy: 0.9694 - val_loss: 1.3428 - val_accuracy: 0.7074\n",
      "Epoch 48/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2949 - accuracy: 0.9803\n",
      "Epoch 00048: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2940 - accuracy: 0.9802 - val_loss: 1.0944 - val_accuracy: 0.8034\n",
      "Epoch 49/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2707 - accuracy: 0.9815\n",
      "Epoch 00049: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2748 - accuracy: 0.9795 - val_loss: 1.4999 - val_accuracy: 0.7540\n",
      "Epoch 50/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3370 - accuracy: 0.9593\n",
      "Epoch 00050: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3447 - accuracy: 0.9574 - val_loss: 1.4221 - val_accuracy: 0.8103\n",
      "Epoch 51/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3704 - accuracy: 0.9605\n",
      "Epoch 00051: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3749 - accuracy: 0.9589 - val_loss: 1.7162 - val_accuracy: 0.7358\n",
      "Epoch 52/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4251 - accuracy: 0.9548\n",
      "Epoch 00052: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.4221 - accuracy: 0.9562 - val_loss: 1.2447 - val_accuracy: 0.7801\n",
      "Epoch 53/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3841 - accuracy: 0.9799\n",
      "Epoch 00053: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3866 - accuracy: 0.9795 - val_loss: 2.0287 - val_accuracy: 0.7465\n",
      "Epoch 54/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3524 - accuracy: 0.9840\n",
      "Epoch 00054: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3559 - accuracy: 0.9833 - val_loss: 1.3355 - val_accuracy: 0.7856\n",
      "Epoch 55/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3493 - accuracy: 0.9815\n",
      "Epoch 00055: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3525 - accuracy: 0.9818 - val_loss: 1.2990 - val_accuracy: 0.8006\n",
      "Epoch 56/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3023 - accuracy: 0.9885\n",
      "Epoch 00056: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3028 - accuracy: 0.9884 - val_loss: 1.3534 - val_accuracy: 0.7824\n",
      "Epoch 57/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2718 - accuracy: 0.9868\n",
      "Epoch 00057: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 430us/sample - loss: 0.2716 - accuracy: 0.9861 - val_loss: 1.1864 - val_accuracy: 0.8113\n",
      "Epoch 58/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2531 - accuracy: 0.9893\n",
      "Epoch 00058: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2529 - accuracy: 0.9895 - val_loss: 1.5653 - val_accuracy: 0.8015\n",
      "Epoch 59/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2660 - accuracy: 0.9860\n",
      "Epoch 00059: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2655 - accuracy: 0.9864 - val_loss: 1.8651 - val_accuracy: 0.7120\n",
      "Epoch 60/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2659 - accuracy: 0.9868\n",
      "Epoch 00060: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.2682 - accuracy: 0.9857 - val_loss: 1.5022 - val_accuracy: 0.7349\n",
      "Epoch 61/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2839 - accuracy: 0.9811\n",
      "Epoch 00061: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 423us/sample - loss: 0.2829 - accuracy: 0.9818 - val_loss: 1.2733 - val_accuracy: 0.7996\n",
      "Epoch 62/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3107 - accuracy: 0.9774\n",
      "Epoch 00062: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3096 - accuracy: 0.9779 - val_loss: 1.1048 - val_accuracy: 0.7903\n",
      "Epoch 63/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3430 - accuracy: 0.9733\n",
      "Epoch 00063: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3440 - accuracy: 0.9721 - val_loss: 1.3517 - val_accuracy: 0.7931\n",
      "Epoch 64/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3347 - accuracy: 0.9762\n",
      "Epoch 00064: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.3318 - accuracy: 0.9771 - val_loss: 1.7166 - val_accuracy: 0.7866\n",
      "Epoch 65/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2970 - accuracy: 0.9868\n",
      "Epoch 00065: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2965 - accuracy: 0.9872 - val_loss: 1.3244 - val_accuracy: 0.7884\n",
      "Epoch 66/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2643 - accuracy: 0.9914\n",
      "Epoch 00066: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 432us/sample - loss: 0.2636 - accuracy: 0.9915 - val_loss: 1.3355 - val_accuracy: 0.7978\n",
      "Epoch 67/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2355 - accuracy: 0.9938\n",
      "Epoch 00067: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2349 - accuracy: 0.9938 - val_loss: 1.5177 - val_accuracy: 0.7665\n",
      "Epoch 68/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2186 - accuracy: 0.9930\n",
      "Epoch 00068: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 433us/sample - loss: 0.2172 - accuracy: 0.9934 - val_loss: 1.3117 - val_accuracy: 0.8052\n",
      "Epoch 69/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1940 - accuracy: 0.9975\n",
      "Epoch 00069: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1943 - accuracy: 0.9973 - val_loss: 1.2521 - val_accuracy: 0.8071\n",
      "Epoch 70/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1795 - accuracy: 0.9975\n",
      "Epoch 00070: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1785 - accuracy: 0.9977 - val_loss: 1.2700 - val_accuracy: 0.8001\n",
      "Epoch 71/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1643 - accuracy: 0.9967\n",
      "Epoch 00071: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1636 - accuracy: 0.9969 - val_loss: 1.4097 - val_accuracy: 0.7973\n",
      "Epoch 72/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1551 - accuracy: 0.9959\n",
      "Epoch 00072: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1544 - accuracy: 0.9961 - val_loss: 1.4335 - val_accuracy: 0.7889\n",
      "Epoch 73/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1452 - accuracy: 0.9971\n",
      "Epoch 00073: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1446 - accuracy: 0.9973 - val_loss: 1.6405 - val_accuracy: 0.7684\n",
      "Epoch 74/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.9975\n",
      "Epoch 00074: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1308 - accuracy: 0.9977 - val_loss: 1.4028 - val_accuracy: 0.7856\n",
      "Epoch 75/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1180 - accuracy: 0.9979\n",
      "Epoch 00075: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1174 - accuracy: 0.9981 - val_loss: 1.4090 - val_accuracy: 0.8094\n",
      "Epoch 76/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9988\n",
      "Epoch 00076: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.1057 - accuracy: 0.9988 - val_loss: 1.4503 - val_accuracy: 0.8164\n",
      "Epoch 77/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0974 - accuracy: 0.9988\n",
      "Epoch 00077: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0969 - accuracy: 0.9988 - val_loss: 1.3776 - val_accuracy: 0.7945\n",
      "Epoch 78/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0901 - accuracy: 0.9988\n",
      "Epoch 00078: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0902 - accuracy: 0.9985 - val_loss: 1.4771 - val_accuracy: 0.7880\n",
      "Epoch 79/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0838 - accuracy: 0.9992\n",
      "Epoch 00079: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0838 - accuracy: 0.9992 - val_loss: 1.3645 - val_accuracy: 0.8122\n",
      "Epoch 80/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0836 - accuracy: 0.9975\n",
      "Epoch 00080: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0831 - accuracy: 0.9977 - val_loss: 1.4733 - val_accuracy: 0.8271\n",
      "Epoch 81/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0886 - accuracy: 0.9963\n",
      "Epoch 00081: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.0894 - accuracy: 0.9961 - val_loss: 1.2891 - val_accuracy: 0.8197\n",
      "Epoch 82/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1607 - accuracy: 0.9860\n",
      "Epoch 00082: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.1683 - accuracy: 0.9837 - val_loss: 1.6026 - val_accuracy: 0.7740\n",
      "Epoch 83/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3035 - accuracy: 0.9650\n",
      "Epoch 00083: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3070 - accuracy: 0.9644 - val_loss: 1.7352 - val_accuracy: 0.7251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4178 - accuracy: 0.9659\n",
      "Epoch 00084: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.4178 - accuracy: 0.9663 - val_loss: 1.3308 - val_accuracy: 0.7936\n",
      "Epoch 85/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4549 - accuracy: 0.9712\n",
      "Epoch 00085: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.4546 - accuracy: 0.9717 - val_loss: 1.7084 - val_accuracy: 0.7745\n",
      "Epoch 86/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4090 - accuracy: 0.9762\n",
      "Epoch 00086: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.4095 - accuracy: 0.9760 - val_loss: 1.7872 - val_accuracy: 0.7377\n",
      "Epoch 87/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3640 - accuracy: 0.9844\n",
      "Epoch 00087: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3651 - accuracy: 0.9837 - val_loss: 1.4969 - val_accuracy: 0.7773\n",
      "Epoch 88/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3424 - accuracy: 0.9848\n",
      "Epoch 00088: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3417 - accuracy: 0.9849 - val_loss: 1.4406 - val_accuracy: 0.7549\n",
      "Epoch 89/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3151 - accuracy: 0.9893\n",
      "Epoch 00089: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 422us/sample - loss: 0.3130 - accuracy: 0.9895 - val_loss: 1.4483 - val_accuracy: 0.7964\n",
      "Epoch 90/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2681 - accuracy: 0.9959\n",
      "Epoch 00090: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.2666 - accuracy: 0.9961 - val_loss: 1.5736 - val_accuracy: 0.7903\n",
      "Epoch 91/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2303 - accuracy: 0.9975\n",
      "Epoch 00091: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2299 - accuracy: 0.9973 - val_loss: 1.4005 - val_accuracy: 0.8071\n",
      "Epoch 92/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1953 - accuracy: 0.9979\n",
      "Epoch 00092: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1942 - accuracy: 0.9981 - val_loss: 1.4921 - val_accuracy: 0.8010\n",
      "Epoch 93/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1722 - accuracy: 0.9984\n",
      "Epoch 00093: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1725 - accuracy: 0.9981 - val_loss: 1.3621 - val_accuracy: 0.7936\n",
      "Epoch 94/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1481 - accuracy: 0.9988\n",
      "Epoch 00094: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1477 - accuracy: 0.9988 - val_loss: 1.3724 - val_accuracy: 0.8127\n",
      "Epoch 95/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.1263 - accuracy: 1.0000 - val_loss: 1.3440 - val_accuracy: 0.8075\n",
      "Epoch 96/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9992\n",
      "Epoch 00096: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1098 - accuracy: 0.9992 - val_loss: 1.3347 - val_accuracy: 0.7940\n",
      "Epoch 97/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0965 - accuracy: 0.9988\n",
      "Epoch 00097: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.0962 - accuracy: 0.9988 - val_loss: 1.2476 - val_accuracy: 0.8122\n",
      "Epoch 98/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0894 - accuracy: 0.9988\n",
      "Epoch 00098: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0893 - accuracy: 0.9988 - val_loss: 1.3239 - val_accuracy: 0.7861\n",
      "Epoch 99/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0845 - accuracy: 0.9992\n",
      "Epoch 00099: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0842 - accuracy: 0.9992 - val_loss: 1.2075 - val_accuracy: 0.8197\n",
      "Epoch 100/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0802 - accuracy: 0.9996\n",
      "Epoch 00100: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.0800 - accuracy: 0.9996 - val_loss: 1.3829 - val_accuracy: 0.7861\n",
      "Epoch 101/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0751 - accuracy: 0.9988\n",
      "Epoch 00101: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0747 - accuracy: 0.9988 - val_loss: 1.2742 - val_accuracy: 0.8048\n",
      "Epoch 102/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0745 - accuracy: 0.9984\n",
      "Epoch 00102: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0746 - accuracy: 0.9985 - val_loss: 1.1484 - val_accuracy: 0.8211\n",
      "Epoch 103/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0708 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0707 - accuracy: 1.0000 - val_loss: 1.2650 - val_accuracy: 0.8006\n",
      "Epoch 104/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0615 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0617 - accuracy: 0.9996 - val_loss: 1.1613 - val_accuracy: 0.8215\n",
      "Epoch 105/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0560 - accuracy: 0.9992\n",
      "Epoch 00105: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0561 - accuracy: 0.9992 - val_loss: 1.2218 - val_accuracy: 0.7996\n",
      "Epoch 106/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0525 - accuracy: 0.9996\n",
      "Epoch 00106: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0526 - accuracy: 0.9996 - val_loss: 1.2406 - val_accuracy: 0.8183\n",
      "Epoch 107/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0508 - accuracy: 0.9996\n",
      "Epoch 00107: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0508 - accuracy: 0.9996 - val_loss: 1.1375 - val_accuracy: 0.8239\n",
      "Epoch 108/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0471 - accuracy: 0.9996\n",
      "Epoch 00108: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0471 - accuracy: 0.9996 - val_loss: 1.2277 - val_accuracy: 0.7982\n",
      "Epoch 109/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0559 - accuracy: 0.9971\n",
      "Epoch 00109: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 423us/sample - loss: 0.0558 - accuracy: 0.9973 - val_loss: 1.4464 - val_accuracy: 0.7651\n",
      "Epoch 110/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0736 - accuracy: 0.9963\n",
      "Epoch 00110: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0744 - accuracy: 0.9965 - val_loss: 1.1415 - val_accuracy: 0.8127\n",
      "Epoch 111/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0798 - accuracy: 0.9984\n",
      "Epoch 00111: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0799 - accuracy: 0.9985 - val_loss: 1.1493 - val_accuracy: 0.7815\n",
      "Epoch 112/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9905\n",
      "Epoch 00112: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1204 - accuracy: 0.9903 - val_loss: 1.2683 - val_accuracy: 0.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1836 - accuracy: 0.9848\n",
      "Epoch 00113: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1832 - accuracy: 0.9853 - val_loss: 1.9260 - val_accuracy: 0.7381\n",
      "Epoch 114/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2454 - accuracy: 0.9831\n",
      "Epoch 00114: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2455 - accuracy: 0.9837 - val_loss: 1.6054 - val_accuracy: 0.8048\n",
      "Epoch 115/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2696 - accuracy: 0.9827\n",
      "Epoch 00115: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2720 - accuracy: 0.9822 - val_loss: 1.8949 - val_accuracy: 0.7502\n",
      "Epoch 116/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3196 - accuracy: 0.9745\n",
      "Epoch 00116: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.3268 - accuracy: 0.9713 - val_loss: 1.6101 - val_accuracy: 0.7647\n",
      "Epoch 117/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3511 - accuracy: 0.9782\n",
      "Epoch 00117: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3504 - accuracy: 0.9779 - val_loss: 1.4875 - val_accuracy: 0.7502\n",
      "Epoch 118/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3550 - accuracy: 0.9799\n",
      "Epoch 00118: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3594 - accuracy: 0.9791 - val_loss: 1.7902 - val_accuracy: 0.7675\n",
      "Epoch 119/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3444 - accuracy: 0.9860\n",
      "Epoch 00119: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3459 - accuracy: 0.9853 - val_loss: 1.5902 - val_accuracy: 0.7833\n",
      "Epoch 120/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3851 - accuracy: 0.9737\n",
      "Epoch 00120: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3813 - accuracy: 0.9748 - val_loss: 1.7775 - val_accuracy: 0.7898\n",
      "Epoch 121/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3764 - accuracy: 0.9762\n",
      "Epoch 00121: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3773 - accuracy: 0.9756 - val_loss: 1.7125 - val_accuracy: 0.7745\n",
      "Epoch 122/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3269 - accuracy: 0.9918\n",
      "Epoch 00122: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.3290 - accuracy: 0.9911 - val_loss: 1.8684 - val_accuracy: 0.7269\n",
      "Epoch 123/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.9922\n",
      "Epoch 00123: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3063 - accuracy: 0.9919 - val_loss: 2.5610 - val_accuracy: 0.6030\n",
      "Epoch 124/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3279 - accuracy: 0.9766\n",
      "Epoch 00124: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.3275 - accuracy: 0.9771 - val_loss: 2.3194 - val_accuracy: 0.6486\n",
      "Epoch 125/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9868\n",
      "Epoch 00125: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3009 - accuracy: 0.9872 - val_loss: 1.7804 - val_accuracy: 0.7521\n",
      "Epoch 126/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3121 - accuracy: 0.9836\n",
      "Epoch 00126: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.3117 - accuracy: 0.9837 - val_loss: 1.6283 - val_accuracy: 0.7661\n",
      "Epoch 127/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3471 - accuracy: 0.9708\n",
      "Epoch 00127: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.3485 - accuracy: 0.9706 - val_loss: 1.9522 - val_accuracy: 0.7754\n",
      "Epoch 128/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4557 - accuracy: 0.9544\n",
      "Epoch 00128: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.4597 - accuracy: 0.9539 - val_loss: 1.9490 - val_accuracy: 0.7600\n",
      "Epoch 129/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4431 - accuracy: 0.9729\n",
      "Epoch 00129: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.4444 - accuracy: 0.9725 - val_loss: 1.6172 - val_accuracy: 0.7241\n",
      "Epoch 130/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.4407 - accuracy: 0.9774\n",
      "Epoch 00130: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.4404 - accuracy: 0.9771 - val_loss: 1.9131 - val_accuracy: 0.7036\n",
      "Epoch 131/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3890 - accuracy: 0.9881\n",
      "Epoch 00131: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3864 - accuracy: 0.9884 - val_loss: 1.5501 - val_accuracy: 0.8006\n",
      "Epoch 132/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.3333 - accuracy: 0.9938\n",
      "Epoch 00132: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.3323 - accuracy: 0.9938 - val_loss: 1.5095 - val_accuracy: 0.8145\n",
      "Epoch 133/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2941 - accuracy: 0.9942\n",
      "Epoch 00133: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2919 - accuracy: 0.9946 - val_loss: 1.4299 - val_accuracy: 0.8010\n",
      "Epoch 134/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2482 - accuracy: 0.9975\n",
      "Epoch 00134: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2475 - accuracy: 0.9973 - val_loss: 1.3353 - val_accuracy: 0.8243\n",
      "Epoch 135/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2133 - accuracy: 0.9992\n",
      "Epoch 00135: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2133 - accuracy: 0.9988 - val_loss: 1.3042 - val_accuracy: 0.8234\n",
      "Epoch 136/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1951 - accuracy: 0.9975\n",
      "Epoch 00136: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.1945 - accuracy: 0.9973 - val_loss: 1.3455 - val_accuracy: 0.8080\n",
      "Epoch 137/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.9967\n",
      "Epoch 00137: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.1862 - accuracy: 0.9965 - val_loss: 1.3791 - val_accuracy: 0.7796\n",
      "Epoch 138/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2573 - accuracy: 0.9790\n",
      "Epoch 00138: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2595 - accuracy: 0.9783 - val_loss: 1.8019 - val_accuracy: 0.7819\n",
      "Epoch 139/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.9881\n",
      "Epoch 00139: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.2495 - accuracy: 0.9884 - val_loss: 2.1967 - val_accuracy: 0.7740\n",
      "Epoch 140/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2282 - accuracy: 0.9955\n",
      "Epoch 00140: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.2285 - accuracy: 0.9946 - val_loss: 1.5592 - val_accuracy: 0.8141\n",
      "Epoch 141/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2227 - accuracy: 0.9922\n",
      "Epoch 00141: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2227 - accuracy: 0.9919 - val_loss: 1.5149 - val_accuracy: 0.8159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2111 - accuracy: 0.9951\n",
      "Epoch 00142: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.2120 - accuracy: 0.9946 - val_loss: 1.6937 - val_accuracy: 0.8131\n",
      "Epoch 143/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.2051 - accuracy: 0.9942\n",
      "Epoch 00143: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.2087 - accuracy: 0.9934 - val_loss: 1.4012 - val_accuracy: 0.8150\n",
      "Epoch 144/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1900 - accuracy: 0.9967\n",
      "Epoch 00144: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1891 - accuracy: 0.9969 - val_loss: 1.4392 - val_accuracy: 0.7819\n",
      "Epoch 145/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1683 - accuracy: 0.9971\n",
      "Epoch 00145: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1695 - accuracy: 0.9965 - val_loss: 1.2345 - val_accuracy: 0.8113\n",
      "Epoch 146/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1593 - accuracy: 0.9951\n",
      "Epoch 00146: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1612 - accuracy: 0.9950 - val_loss: 1.4342 - val_accuracy: 0.7968\n",
      "Epoch 147/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.9971\n",
      "Epoch 00147: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.1472 - accuracy: 0.9973 - val_loss: 1.3314 - val_accuracy: 0.8178\n",
      "Epoch 148/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1332 - accuracy: 0.9984\n",
      "Epoch 00148: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.1326 - accuracy: 0.9985 - val_loss: 1.3477 - val_accuracy: 0.8108\n",
      "Epoch 149/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9979\n",
      "Epoch 00149: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.1228 - accuracy: 0.9981 - val_loss: 1.2058 - val_accuracy: 0.8267\n",
      "Epoch 150/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1086 - accuracy: 0.9988\n",
      "Epoch 00150: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.1083 - accuracy: 0.9988 - val_loss: 1.3732 - val_accuracy: 0.8010\n",
      "Epoch 151/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.1023 - accuracy: 0.9988\n",
      "Epoch 00151: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.1018 - accuracy: 0.9988 - val_loss: 1.2185 - val_accuracy: 0.8178\n",
      "Epoch 152/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0910 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0907 - accuracy: 1.0000 - val_loss: 1.2623 - val_accuracy: 0.8201\n",
      "Epoch 153/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0783 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 429us/sample - loss: 0.0782 - accuracy: 1.0000 - val_loss: 1.2358 - val_accuracy: 0.8169\n",
      "Epoch 154/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0687 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.0684 - accuracy: 1.0000 - val_loss: 1.1944 - val_accuracy: 0.8183\n",
      "Epoch 155/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0613 - accuracy: 0.9996\n",
      "Epoch 00155: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0611 - accuracy: 0.9996 - val_loss: 1.2020 - val_accuracy: 0.8183\n",
      "Epoch 156/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0548 - accuracy: 1.0000 - val_loss: 1.1879 - val_accuracy: 0.8145\n",
      "Epoch 157/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0505 - accuracy: 1.0000 - val_loss: 1.1627 - val_accuracy: 0.8192\n",
      "Epoch 158/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.0455 - accuracy: 1.0000 - val_loss: 1.1582 - val_accuracy: 0.8197\n",
      "Epoch 159/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9996\n",
      "Epoch 00159: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0414 - accuracy: 0.9996 - val_loss: 1.1345 - val_accuracy: 0.8234\n",
      "Epoch 160/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0390 - accuracy: 0.9992\n",
      "Epoch 00160: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 428us/sample - loss: 0.0388 - accuracy: 0.9992 - val_loss: 1.1173 - val_accuracy: 0.8243\n",
      "Epoch 161/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0372 - accuracy: 0.9992\n",
      "Epoch 00161: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0372 - accuracy: 0.9992 - val_loss: 1.1250 - val_accuracy: 0.8131\n",
      "Epoch 162/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0378 - accuracy: 0.9988\n",
      "Epoch 00162: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0376 - accuracy: 0.9988 - val_loss: 1.0567 - val_accuracy: 0.8234\n",
      "Epoch 163/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0430 - accuracy: 0.9988\n",
      "Epoch 00163: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0429 - accuracy: 0.9988 - val_loss: 1.0443 - val_accuracy: 0.8281\n",
      "Epoch 164/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0481 - accuracy: 0.9988\n",
      "Epoch 00164: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0483 - accuracy: 0.9988 - val_loss: 1.1008 - val_accuracy: 0.8262\n",
      "Epoch 165/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0504 - accuracy: 0.9996\n",
      "Epoch 00165: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 429us/sample - loss: 0.0503 - accuracy: 0.9996 - val_loss: 1.0872 - val_accuracy: 0.8034\n",
      "Epoch 166/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0492 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0491 - accuracy: 1.0000 - val_loss: 1.1063 - val_accuracy: 0.8304\n",
      "Epoch 167/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0441 - accuracy: 0.9996\n",
      "Epoch 00167: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0439 - accuracy: 0.9996 - val_loss: 1.1376 - val_accuracy: 0.8150\n",
      "Epoch 168/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0367 - accuracy: 1.0000 - val_loss: 1.1358 - val_accuracy: 0.8187\n",
      "Epoch 169/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0311 - accuracy: 1.0000 - val_loss: 1.0916 - val_accuracy: 0.8262\n",
      "Epoch 170/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0265 - accuracy: 1.0000 - val_loss: 1.0900 - val_accuracy: 0.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0236 - accuracy: 1.0000 - val_loss: 1.0849 - val_accuracy: 0.8192\n",
      "Epoch 172/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0214 - accuracy: 1.0000 - val_loss: 1.0462 - val_accuracy: 0.8187\n",
      "Epoch 173/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0205 - accuracy: 1.0000 - val_loss: 1.0451 - val_accuracy: 0.8159\n",
      "Epoch 174/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 429us/sample - loss: 0.0195 - accuracy: 1.0000 - val_loss: 1.0204 - val_accuracy: 0.8164\n",
      "Epoch 175/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 436us/sample - loss: 0.0185 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.8220\n",
      "Epoch 176/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9996\n",
      "Epoch 00176: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 430us/sample - loss: 0.0192 - accuracy: 0.9996 - val_loss: 1.1612 - val_accuracy: 0.7926\n",
      "Epoch 177/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0280 - accuracy: 0.9992\n",
      "Epoch 00177: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 433us/sample - loss: 0.0282 - accuracy: 0.9992 - val_loss: 1.0388 - val_accuracy: 0.8122\n",
      "Epoch 178/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0374 - accuracy: 0.9992\n",
      "Epoch 00178: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0378 - accuracy: 0.9992 - val_loss: 1.2093 - val_accuracy: 0.8187\n",
      "Epoch 179/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9996\n",
      "Epoch 00179: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0409 - accuracy: 0.9996 - val_loss: 1.1172 - val_accuracy: 0.8285\n",
      "Epoch 180/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0389 - accuracy: 0.9996\n",
      "Epoch 00180: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0387 - accuracy: 0.9996 - val_loss: 1.1703 - val_accuracy: 0.8113\n",
      "Epoch 181/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0358 - accuracy: 0.9996\n",
      "Epoch 00181: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0357 - accuracy: 0.9996 - val_loss: 1.0096 - val_accuracy: 0.8215\n",
      "Epoch 182/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 00182: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.9930 - val_accuracy: 0.8243\n",
      "Epoch 183/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0297 - accuracy: 1.0000 - val_loss: 1.0916 - val_accuracy: 0.8080\n",
      "Epoch 184/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 00184: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 430us/sample - loss: 0.0266 - accuracy: 1.0000 - val_loss: 1.0813 - val_accuracy: 0.8145\n",
      "Epoch 185/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 00185: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 421us/sample - loss: 0.0222 - accuracy: 1.0000 - val_loss: 1.0702 - val_accuracy: 0.8173\n",
      "Epoch 186/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0190 - accuracy: 1.0000 - val_loss: 1.0228 - val_accuracy: 0.8169\n",
      "Epoch 187/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0170 - accuracy: 1.0000 - val_loss: 1.0886 - val_accuracy: 0.8071\n",
      "Epoch 188/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 00188: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 1.0375 - val_accuracy: 0.8187\n",
      "Epoch 189/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.0306 - val_accuracy: 0.8183\n",
      "Epoch 190/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.9922 - val_accuracy: 0.8239\n",
      "Epoch 191/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 423us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.9834 - val_accuracy: 0.8271\n",
      "Epoch 192/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 00192: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.9685 - val_accuracy: 0.8239\n",
      "Epoch 193/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.9514 - val_accuracy: 0.8332\n",
      "Epoch 194/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.9935 - val_accuracy: 0.8253\n",
      "Epoch 195/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.9744 - val_accuracy: 0.8239\n",
      "Epoch 196/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0184 - accuracy: 0.9996\n",
      "Epoch 00196: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 424us/sample - loss: 0.0185 - accuracy: 0.9996 - val_loss: 0.9717 - val_accuracy: 0.8322\n",
      "Epoch 197/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0281 - accuracy: 0.9988\n",
      "Epoch 00197: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0282 - accuracy: 0.9988 - val_loss: 1.1813 - val_accuracy: 0.8201\n",
      "Epoch 198/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0359 - accuracy: 0.9992\n",
      "Epoch 00198: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 426us/sample - loss: 0.0358 - accuracy: 0.9992 - val_loss: 1.1049 - val_accuracy: 0.7973\n",
      "Epoch 199/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 00199: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 425us/sample - loss: 0.0371 - accuracy: 0.9996 - val_loss: 1.2578 - val_accuracy: 0.8001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "2432/2582 [===========================>..] - ETA: 0s - loss: 0.0595 - accuracy: 0.9951\n",
      "Epoch 00200: val_loss did not improve from 0.82465\n",
      "2582/2582 [==============================] - 1s 427us/sample - loss: 0.0612 - accuracy: 0.9950 - val_loss: 1.2706 - val_accuracy: 0.8239\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=x_training, y=y_training_encoded, batch_size=128, epochs=200, verbose=1, callbacks=[checkpoint],\n",
    "     validation_data=(x_testing, y_testing_encoded), shuffle=True, initial_epoch=0, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x = x_training, y = y_training_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "2146/1 [============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.5745 - accuracy: 0.8239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0790586424201642, 0.8238583]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x = x_testing, y = y_testing_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/134-1.611.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (150, 150))\n",
    "    img = np.expand_dims(img, -1)\n",
    "    img = img / 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testing[150].shape, y_testing_encoded[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = read_image('test_images/asmaa/happuy.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_image[:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(np.array([test_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'anger': 0,\n",
    " 'surprise': 1,\n",
    " 'disgust': 2,\n",
    " 'neutral': 3,\n",
    " 'happiness': 4,\n",
    " 'sadness': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_image * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
